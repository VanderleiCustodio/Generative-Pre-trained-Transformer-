# Generative-Pre-trained-Transformer-
An educational and experimental project that implements the core components of a Transformer from scratch â€” including embeddings, self-attention, feed-forward networks, normalization layers, and autoregressive text generation. The goal is to provide a clear, step-by-step understanding of how GPT-style language models work internally
