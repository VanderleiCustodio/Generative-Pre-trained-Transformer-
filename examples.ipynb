{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Configurações\n",
    "vocab_size = 96  # Apenas 10 palavras no nosso dicionário\n",
    "block_size = 5   # Tamanho máximo da frase (Time)\n",
    "n_embd = 32      # Tamanho do vetor de características (Channels)\n",
    "\n",
    "# Criando as tabelas\n",
    "token_table = nn.Embedding(vocab_size, n_embd) ## 10 \n",
    "\n",
    "pos_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "# Simulação de entrada (Batch=1, Time=3) -> IDs: 2, 5, 1\n",
    "idx = torch.tensor([[2, 5, 1]]) \n",
    "\n",
    "T = idx.shape[1] # T = 3\n",
    "\n",
    "print(\"--- 1. Token Embeddings ---\")\n",
    "tok_emb = token_table(idx)\n",
    "print(f\"Shape: {tok_emb.shape}\") \n",
    "# Esperado: torch.Size([1, 3, 4])\n",
    "print(\"Valores (exemplo do primeiro token):\\n\", tok_emb)\n",
    "\n",
    "print(\"\\n--- 2. Positional Embeddings ---\")\n",
    "# Cria vetor [0, 1, 2]\n",
    "\n",
    "positions = torch.arange(T) \n",
    "pos_emb = pos_table(positions)\n",
    "print(f\"Indices de posição: {positions}\")\n",
    "print(f\"Shape: {pos_emb.shape}\") \n",
    "# Esperado: torch.Size([3, 4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lolbas.txt\",'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "    vocab_Size = len(chars)\n",
    "    print(vocab_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[-0.7511, -0.0145,  0.6098,  0.2055],\n",
      "         [ 0.4603,  1.3308, -0.1973, -0.7487],\n",
      "         [ 0.2193,  0.3633, -0.0208,  0.7879],\n",
      "         [ 0.2667, -0.4752,  0.2586,  0.0215],\n",
      "         [ 1.0746,  0.8932, -0.3398, -0.6778],\n",
      "         [ 1.3327, -0.6830, -1.0598, -0.0645],\n",
      "         [-0.0712,  0.5277,  0.1527, -0.1633],\n",
      "         [ 1.3867,  0.0329, -0.8127, -0.1333],\n",
      "         [ 0.9844,  0.4528,  0.0932, -0.1519],\n",
      "         [ 0.5773,  1.1063, -0.1016, -0.5399]]], grad_fn=<ViewBackward0>), tensor([[[ 0.2372,  0.6217,  0.0210,  0.6227],\n",
      "         [ 1.0184,  0.3353, -0.4608, -0.6644],\n",
      "         [ 0.4604,  0.5118, -0.1035,  0.7490],\n",
      "         [-0.1434, -1.0228, -0.2137, -0.4070],\n",
      "         [ 0.4483, -0.1489,  0.9989,  0.7128],\n",
      "         [ 0.1009, -0.2841, -0.3198, -0.2053],\n",
      "         [ 0.1754,  0.1336, -0.1278,  0.6057],\n",
      "         [-0.2005,  0.0565,  1.4174,  0.2233],\n",
      "         [ 0.8746, -0.4146, -0.1881, -0.1749],\n",
      "         [ 0.7901, -0.1415, -0.4115, -0.1692]]], grad_fn=<ViewBackward0>), tensor([[[ 0.6350,  0.2678, -0.3064,  0.0158],\n",
      "         [ 0.3027, -0.2310, -0.7606, -1.2072],\n",
      "         [-0.9900, -0.6993, -1.0600,  0.3478],\n",
      "         [ 0.8919, -0.6382,  0.5294, -0.1043],\n",
      "         [ 0.4474, -0.2419, -1.2280, -0.2806],\n",
      "         [-0.4375,  0.8375,  0.5620, -1.4621],\n",
      "         [-0.8481, -0.4688, -0.1868,  0.4478],\n",
      "         [-0.0042,  0.4084, -1.1399, -0.9883],\n",
      "         [ 0.1210, -0.2865, -0.3627, -0.1969],\n",
      "         [-0.4027, -0.2507, -0.3787, -0.2821]]], grad_fn=<ViewBackward0>)]\n",
      "tensor([[[-0.7511, -0.0145,  0.6098,  0.2055,  0.2372,  0.6217,  0.0210,\n",
      "           0.6227,  0.6350,  0.2678, -0.3064,  0.0158],\n",
      "         [ 0.4603,  1.3308, -0.1973, -0.7487,  1.0184,  0.3353, -0.4608,\n",
      "          -0.6644,  0.3027, -0.2310, -0.7606, -1.2072],\n",
      "         [ 0.2193,  0.3633, -0.0208,  0.7879,  0.4604,  0.5118, -0.1035,\n",
      "           0.7490, -0.9900, -0.6993, -1.0600,  0.3478],\n",
      "         [ 0.2667, -0.4752,  0.2586,  0.0215, -0.1434, -1.0228, -0.2137,\n",
      "          -0.4070,  0.8919, -0.6382,  0.5294, -0.1043],\n",
      "         [ 1.0746,  0.8932, -0.3398, -0.6778,  0.4483, -0.1489,  0.9989,\n",
      "           0.7128,  0.4474, -0.2419, -1.2280, -0.2806],\n",
      "         [ 1.3327, -0.6830, -1.0598, -0.0645,  0.1009, -0.2841, -0.3198,\n",
      "          -0.2053, -0.4375,  0.8375,  0.5620, -1.4621],\n",
      "         [-0.0712,  0.5277,  0.1527, -0.1633,  0.1754,  0.1336, -0.1278,\n",
      "           0.6057, -0.8481, -0.4688, -0.1868,  0.4478],\n",
      "         [ 1.3867,  0.0329, -0.8127, -0.1333, -0.2005,  0.0565,  1.4174,\n",
      "           0.2233, -0.0042,  0.4084, -1.1399, -0.9883],\n",
      "         [ 0.9844,  0.4528,  0.0932, -0.1519,  0.8746, -0.4146, -0.1881,\n",
      "          -0.1749,  0.1210, -0.2865, -0.3627, -0.1969],\n",
      "         [ 0.5773,  1.1063, -0.1016, -0.5399,  0.7901, -0.1415, -0.4115,\n",
      "          -0.1692, -0.4027, -0.2507, -0.3787, -0.2821]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Shape após CAT: torch.Size([1, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Configurações\n",
    "head_size = 4\n",
    "num_heads = 3\n",
    "n_embd = head_size * num_heads # 12\n",
    "\n",
    "# Simulação da classe Head (simplificada)\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        # Cada cabeça projeta para um tamanho pequeno\n",
    "        self.linear = nn.Linear(n_embd, size) ## -- >> 12, \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Nossa classe MultiHead (Explodida para teste)\n",
    "class MultiHeadSim(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Cria 3 cabeças independentes\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Projeção final para misturar\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x): # Forward escrito corretamente!\n",
    "        # 1. Cada cabeça processa\n",
    "        head_outputs = [h(x) for h in self.heads]\n",
    "        print(head_outputs.shape)\n",
    "        \n",
    "        # 2. Concatenamos (Cole os vetores lado a lado)\n",
    "        out = torch.cat(head_outputs, dim=-1)\n",
    "        print(out)\n",
    "        print(f\"Shape após CAT: {out.shape}\") # Deve ser (Batch, T, 12)\n",
    "        \n",
    "        # 3. Projeção\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "# Teste\n",
    "model = MultiHeadSim()\n",
    "x = torch.randn(1, 10, n_embd) # (Batch, Time, Channels=12)\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce4b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
